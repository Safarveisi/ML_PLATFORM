---
apiVersion: v1
kind: Secret
metadata:
  name: s3creds
  namespace: default
  annotations:
     serving.kserve.io/s3-endpoint: ${S3_ENDPOINT_URL}
     serving.kserve.io/s3-usehttps: "1"
     serving.kserve.io/s3-region: ${S3_REGION}
     serving.kserve.io/s3-useanoncredential: "false"
type: Opaque
stringData:
  AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
  AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: sa
  namespace: default
  annotations:
     serving.kserve.io/s3-endpoint: ${S3_ENDPOINT_URL}
     serving.kserve.io/s3-usehttps: "1"
     serving.kserve.io/s3-region: ${S3_REGION}
     serving.kserve.io/s3-useanoncredential: "false"
secrets:
  - name: s3creds
---
apiVersion: "serving.kserve.io/v1beta1"
kind: InferenceService
metadata:
  name: iris-classifier
  namespace: default
spec:
  predictor:
    containerConcurrency: 10 # Ten requests per pod (InferenceService replica)
    minReplicas: 1 # This prevents cold start 
    maxReplicas: 3 # Maximum number of pods (surplus requests will be buffered)
    serviceAccountName: sa
    # canaryTrafficPercent: 10 # The canary model
    model:
      modelFormat:
        name: mlflow
      protocolVersion: v2
      storageUri: s3://customerintelligence/${MLFLOW_ARTIFACTORY_PREFIX}/${ARTIFACTS_PATH}
      resources: # Resources for each replica
        limits: 
          cpu: "3"
          memory: "5Gi"
        requests:
          cpu: "3"
          memory: "5Gi"